---
title: "R Notebook"
output:
  html_document:
    df_print: paged
  pdf_document: default
  word_document: default
---
Q5) Repartition the data, this time into training, validation, and test sets (50% : 30% : 20%). Apply the k-NN method with the k chosen above. Compare the confusion matrix of the test set with that of the training and validation sets. Comment on the differences and their reason.

*Using Libraries and reading data :-

```{r}

library(caret)
library(dplyr)
library(FNN)

UniversalBank <- read.csv("UniversalBank.csv")
head(UniversalBank)
str(UniversalBank)

```

* Transforming categorical predictors into dummy variables and Elementing the ID and ZIP code:-

```{r}

UniversalBank <- UniversalBank[,c(-1,-5)]
str(UniversalBank)

#Dummy Variables

test_0 <- UniversalBank$Education
Test_factor <- factor(test_0, label =c("E1","E2","E3"))
str(Test_factor)
UniversalBank[["Education"]] <- Test_factor
levels(UniversalBank$Education)

Dummy<- dummyVars("~Education", data=UniversalBank)
y <- predict(Dummy,newdata=UniversalBank)
#print(y)
UniversalBank <- cbind(UniversalBank, y)
UniversalBank <- UniversalBank[, c(-6)]

```

* Partitioning the data into training as 50%, validation as 30% and Test as 20% sets; Normalization of the data :-

```{r}

set.seed(10)
Test_Index <- createDataPartition(UniversalBank$Age,p=0.2, list=FALSE) # 20% Test data
Test_Data <- UniversalBank[Test_Index,]
TraVal_Data <- UniversalBank[-Test_Index,]

Train_Index = createDataPartition(TraVal_Data$Age,p=0.625, list=FALSE) # 50% of remaining data is training and 30% is validation data.
Train_Data = TraVal_Data[Train_Index,]
Validation_data = TraVal_Data[-Train_Index,] 
summary(Train_Data)

#Normalization

Norm<-preProcess(Train_Data, method = c("center","scale"))
Train_Norm<-predict(Norm,Train_Data)
Test_Norm<-predict(Norm,Test_Data)
Validation_Norm<-predict(Norm,Validation_data)
All_Data_Norm<-predict(Norm,UniversalBank)

Train_Predictors<-Train_Norm[,-7]
Test_predictors <- Test_Norm[,-7]
validation_Predictors<- Validation_Norm[,-7]
All_Data_Predictors<- All_Data_Norm[,-7]

Train_labels <-factor(Train_Data[,7], levels=c(0,1), labels = c("NO","YES"))
Validation_labels  <-factor(Validation_data[,7], levels=c(0,1), labels = c("NO","YES"))
Test_labels <- factor(Test_Data[,7], levels=c(0,1), labels = c("NO","YES"))
All_Data_labels <- factor(UniversalBank[,7], levels=c(0,1), labels = c("NO","YES"))

```

* k-NN classification using k=3 :-
```{r}

PredictedTest_labels <- knn(Train_Predictors, Test_predictors, cl=Train_labels, k=3, prob = TRUE)
str(PredictedTest_labels)
PredictedValid_labels <- knn(Train_Predictors, validation_Predictors, cl=Train_labels, k=3, prob = TRUE)
str(PredictedValid_labels)

```

*Confusion matrix:-

```{r}

library(gmodels)
CrossTable(x=Validation_labels, y=PredictedValid_labels, prop.chisq = FALSE)

CrossTable(x=Test_labels, y=PredictedTest_labels, prop.chisq = FALSE)

```
# When data was partitioned as 60% training and 40% as validation data, the hypertuned k value was 3. Using k=3 the accuracy derived was 96.8%. But when the data was partitioned as train 50% validation 30% and test 20%, while using k=3; Acuracy derived for Validation data is 95.86% and testing data is 95.10%. They accuracy might have decreased due to different partition variables.